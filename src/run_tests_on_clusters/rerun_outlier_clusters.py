#!/usr/bin/env python3
"""
rerun_outlier_clusters.py

Script to re-execute tests on clusters that exhibit improvement outliers.

This script:
1. Reads an outlier report generated by find_improvement_outliers.py
2. Extracts unique cluster names that have anomalous improvements
3. Re-executes tests for both base code and LLM-generated code
4. Forces complete re-execution regardless of existing results
5. Generates new raw data to potentially resolve outlier issues

The goal is to obtain fresh, reliable metrics for clusters with anomalous improvements
(e.g., +2000% degradation or -95% improvement) that may be due to measurement errors.


Date: 2025-10-17
"""

import json
import sys
import subprocess
import argparse
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple
from collections import defaultdict
from datetime import datetime
import time

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).resolve().parent.parent))
from utility_dir.utility_paths import (
    METRICS_DIR_FILEPATH,
    OUTPUT_DIR_FILEPATH,
    CLUSTERS_DIR_FILEPATH,
    SRC_DIR,
)


class OutlierClusterRerunner:
    """Main class for re-running tests on clusters with outlier improvements"""

    def __init__(
        self,
        outlier_report_path: Path,
        run_tests_script: Path,
        num_executions: int = 5,
        max_workers: int = 4,
        dry_run: bool = False,
        backup_results: bool = True,
    ):
        """
        Initialize the rerunner

        Args:
            outlier_report_path: Path to the outlier report JSON file
            run_tests_script: Path to run_tests_on_cluster.py
            num_executions: Number of times to execute each cluster (default: 5)
            max_workers: Number of parallel workers for test execution (default: 4)
            dry_run: If True, only show what would be done without executing (default: False)
            backup_results: If True, backup existing results before re-execution (default: True)
        """
        self.outlier_report_path = Path(outlier_report_path)
        self.run_tests_script = Path(run_tests_script)
        self.num_executions = num_executions
        self.max_workers = max_workers
        self.dry_run = dry_run
        self.backup_results = backup_results

        self.outlier_data: Optional[Dict] = None
        self.problematic_clusters: Set[str] = set()
        self.prompt_versions_by_cluster: Dict[str, Set[int]] = defaultdict(set)

        print(f"Initialized OutlierClusterRerunner")
        print(f"  Outlier report: {self.outlier_report_path}")
        print(f"  Test runner script: {self.run_tests_script}")
        print(f"  Number of executions per cluster: {self.num_executions}")
        print(f"  Max parallel workers: {self.max_workers}")
        print(f"  Dry run mode: {self.dry_run}")
        print(f"  Backup existing results: {self.backup_results}")

    def load_outlier_report(self) -> bool:
        """
        Load and parse the outlier report

        Returns:
            True if successful, False otherwise
        """
        if not self.outlier_report_path.exists():
            print(f"Error: Outlier report not found at {self.outlier_report_path}")
            return False

        try:
            with open(self.outlier_report_path, "r", encoding="utf-8") as f:
                self.outlier_data = json.load(f)

            print(f"\nLoaded outlier report successfully")
            print(
                f"  Generation date: {self.outlier_data['metadata']['generation_date']}"
            )
            print(
                f"  Threshold: ±{self.outlier_data['metadata']['threshold_percentage']}%"
            )
            print(
                f"  Total outliers: {self.outlier_data['metadata']['total_outliers']}"
            )

            return True

        except (json.JSONDecodeError, KeyError, IOError) as e:
            print(f"Error loading outlier report: {e}")
            return False

    def extract_problematic_clusters(self) -> Set[str]:
        """
        Extract unique cluster names from outliers

        Returns:
            Set of cluster names that have outliers
        """
        if not self.outlier_data:
            print("Error: No outlier data loaded")
            return set()

        clusters = set()
        prompt_versions = defaultdict(set)

        for outlier in self.outlier_data["outliers"]:
            cluster_name = outlier["cluster_name"]
            clusters.add(cluster_name)

            # Extract prompt version number (e.g., "v1" -> 1)
            prompt_version_str = outlier["prompt_version"]
            if prompt_version_str.startswith("v"):
                try:
                    version_num = int(prompt_version_str[1:])
                    prompt_versions[cluster_name].add(version_num)
                except ValueError:
                    pass

        self.problematic_clusters = clusters
        self.prompt_versions_by_cluster = prompt_versions

        print(f"\nExtracted {len(clusters)} unique clusters with outliers")

        # Show summary statistics
        summary = self.outlier_data.get("summary", {})
        if summary:
            print("\nOutlier distribution:")
            print(f"  By LLM Model: {summary.get('by_llm_model', {})}")
            print(f"  By Prompt Version: {summary.get('by_prompt_version', {})}")
            print(f"  By Metric: {summary.get('by_metric', {})}")
            print(f"  By Language: {summary.get('by_language', {})}")

        return clusters

    def get_cluster_outlier_details(self, cluster_name: str) -> Dict:
        """
        Get detailed outlier information for a specific cluster

        Args:
            cluster_name: Name of the cluster

        Returns:
            Dictionary with outlier statistics for the cluster
        """
        if not self.outlier_data:
            return {}

        cluster_outliers = [
            o
            for o in self.outlier_data["outliers"]
            if o["cluster_name"] == cluster_name
        ]

        details = {
            "total_outliers": len(cluster_outliers),
            "by_llm": defaultdict(int),
            "by_prompt_version": defaultdict(int),
            "by_metric": defaultdict(int),
            "affected_entries": set(),
        }

        for outlier in cluster_outliers:
            details["by_llm"][outlier["llm_model"]] += 1
            details["by_prompt_version"][outlier["prompt_version"]] += 1
            details["by_metric"][outlier["metric_name"]] += 1
            details["affected_entries"].add(outlier["entry_id_base"])

        # Convert defaultdicts to regular dicts for JSON serialization
        details["by_llm"] = dict(details["by_llm"])
        details["by_prompt_version"] = dict(details["by_prompt_version"])
        details["by_metric"] = dict(details["by_metric"])
        details["affected_entries"] = list(details["affected_entries"])

        return details

    def backup_existing_results(self, cluster_name: str) -> bool:
        """
        Backup existing result files for a cluster before re-execution
        Saves backups to src/backup_executions instead of src/execution_outputs

        Args:
            cluster_name: Name of the cluster

        Returns:
            True if backup successful or no files to backup, False on error
        """
        if not self.backup_results:
            return True

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # Use dedicated backup directory in src/backup_executions
        backup_base_dir = SRC_DIR / "backup_executions"
        backup_dir = backup_base_dir / f"backup_{timestamp}_{cluster_name}"

        # Find all result files for this cluster
        result_files = []

        # Base code results: {cluster}_results_{1-5}.json
        for i in range(1, self.num_executions + 1):
            base_file = OUTPUT_DIR_FILEPATH / f"{cluster_name}_results_{i}.json"
            if base_file.exists():
                result_files.append(base_file)

        # LLM results: {cluster}_results_v{1-4}_{1-5}.json
        for version in range(1, 5):
            for i in range(1, self.num_executions + 1):
                llm_file = (
                    OUTPUT_DIR_FILEPATH / f"{cluster_name}_results_v{version}_{i}.json"
                )
                if llm_file.exists():
                    result_files.append(llm_file)

        if not result_files:
            print(f"  No existing results found for {cluster_name} - nothing to backup")
            return True

        try:
            backup_dir.mkdir(parents=True, exist_ok=True)

            import shutil

            for file in result_files:
                backup_file = backup_dir / file.name
                shutil.copy2(file, backup_file)

            print(f"  Backed up {len(result_files)} result files to {backup_dir}")
            return True

        except Exception as e:
            print(f"  Warning: Failed to backup results: {e}")
            return False

    def run_cluster_tests(self, cluster_name: str) -> Tuple[bool, bool]:
        """
        Execute tests for a single cluster (both base and LLM code)

        Args:
            cluster_name: Name of the cluster to test

        Returns:
            Tuple of (base_success, llm_success)
        """
        base_success = False
        llm_success = False

        # Verify cluster file exists
        cluster_file = CLUSTERS_DIR_FILEPATH / f"cluster_{cluster_name}.json"
        if not cluster_file.exists():
            print(f"  Error: Cluster file not found: {cluster_file}")
            return (False, False)

        print(f"\n{'=' * 80}")
        print(f"Re-executing tests for cluster: {cluster_name}")
        print(f"{'=' * 80}")

        # Get outlier details for this cluster
        details = self.get_cluster_outlier_details(cluster_name)
        print(f"  Outliers in this cluster: {details['total_outliers']}")
        print(f"  Affected entries: {len(details['affected_entries'])}")
        print(f"  By metric: {details['by_metric']}")

        if self.dry_run:
            print("  [DRY RUN] Would execute the following commands:")

        # Step 1: Backup existing results
        if self.backup_results and not self.dry_run:
            print("\n  Step 1: Backing up existing results...")
            self.backup_existing_results(cluster_name)
        else:
            print("\n  Step 1: Skipping backup (disabled or dry run)")

        # Step 2: Run base code tests (with outlier-mode for selective execution)
        print(
            f"\n  Step 2: Running base code tests ({self.num_executions} executions)..."
        )
        base_cmd = [
            sys.executable,
            str(self.run_tests_script),
            "--cluster-name",
            cluster_name,
            "--base-only",
            "--run-quantity",
            str(self.num_executions),
            "--max-workers",
            str(self.max_workers),
            "--not-check-pending",  # Force execution even if results exist
            "--overwrite-results",  # Force execution even if results exist
            "--outlier-mode",  # Enable outlier-selective execution
            "--outlier-report",
            str(self.outlier_report_path),  # Path to outlier report
        ]

        if self.dry_run:
            print(f"    Command: {' '.join(base_cmd)}")
            base_success = True
        else:
            try:
                print(f"    Executing: {' '.join(base_cmd)}")
                result = subprocess.run(
                    base_cmd,
                    capture_output=True,
                    text=True,
                    timeout=4 * 3600,  # 120 minute timeout
                )

                if result.returncode == 0:
                    print("    ✓ Base code tests completed successfully")
                    base_success = True
                else:
                    print(
                        f"    ✗ Base code tests failed with exit code {result.returncode}"
                    )
                    if result.stderr:
                        print(f"    Error output: {result.stderr}")

            except subprocess.TimeoutExpired:
                print("    ✗ Base code tests timed out")
            except Exception as e:
                print(f"    ✗ Error running base code tests: {e}")

        # Step 3: Run LLM code tests for all prompt versions
        print(
            f"\n  Step 3: Running LLM code tests (all prompt versions, {self.num_executions} executions each)..."
        )

        # Determine which prompt versions to run
        prompt_versions_to_run = self.prompt_versions_by_cluster.get(
            cluster_name, {1, 2, 3, 4}
        )
        if not prompt_versions_to_run:
            prompt_versions_to_run = {-1}  # Default to all

        print(f"    Prompt versions to run: {sorted(prompt_versions_to_run)}")

        llm_success_count = 0
        for version in sorted(prompt_versions_to_run):
            print(f"\n    Running prompt version v{version}...")

            llm_cmd = [
                sys.executable,
                str(self.run_tests_script),
                "--cluster-name",
                cluster_name,
                "--llm-only",
                "--prompt-version",
                str(version),
                "--run-quantity",
                str(self.num_executions),
                "--max-workers",
                str(self.max_workers),
                "--not-check-pending",
                "--overwrite-results",  # Force overwrite of existing results
                "--outlier-mode",  # Enable outlier-selective execution
                "--outlier-report",
                str(self.outlier_report_path),  # Path to outlier report
            ]

            if self.dry_run:
                print(f"      Command: {' '.join(llm_cmd)}")
                llm_success_count += 1
            else:
                try:
                    print(f"      Executing: {' '.join(llm_cmd)}")
                    result = subprocess.run(
                        llm_cmd,
                        capture_output=True,
                        text=True,
                        timeout=4 * 3600,  # 120 minute timeout for LLM tests
                    )

                    if result.returncode == 0:
                        print(f"      ✓ LLM v{version} tests completed successfully")
                        llm_success_count += 1
                    else:
                        print(
                            f"      ✗ LLM v{version} tests failed with exit code {result.returncode}"
                        )
                        if result.stderr:
                            print(f"      Error output: {result.stderr[:500]}")

                except subprocess.TimeoutExpired:
                    print(f"      ✗ LLM v{version} tests timed out")
                except Exception as e:
                    print(f"      ✗ Error running LLM v{version} tests: {e}")

        llm_success = llm_success_count == len(prompt_versions_to_run)
        if llm_success:
            print(f"\n    ✓ All LLM prompt versions completed successfully")
        else:
            print(
                f"\n    ⚠ {llm_success_count}/{len(prompt_versions_to_run)} LLM prompt versions completed successfully"
            )

        return (base_success, llm_success)

    def rerun_all_problematic_clusters(self) -> Dict[str, Tuple[bool, bool]]:
        """
        Re-execute tests for all problematic clusters

        Returns:
            Dictionary mapping cluster names to (base_success, llm_success) tuples
        """
        if not self.problematic_clusters:
            print("No problematic clusters to re-run")
            return {}

        print(f"\n{'=' * 80}")
        print(
            f"STARTING RE-EXECUTION OF {len(self.problematic_clusters)} PROBLEMATIC CLUSTERS"
        )
        print(f"{'=' * 80}")

        results = {}
        successful_reruns = 0
        start_time = time.time()

        for idx, cluster_name in enumerate(sorted(self.problematic_clusters), 1):
            print(
                f"\n[{idx}/{len(self.problematic_clusters)}] Processing cluster: {cluster_name}"
            )

            base_success, llm_success = self.run_cluster_tests(cluster_name)
            results[cluster_name] = (base_success, llm_success)

            if base_success and llm_success:
                successful_reruns += 1

        elapsed_time = time.time() - start_time
        hours = int(elapsed_time // 3600)
        minutes = int((elapsed_time % 3600) // 60)
        seconds = int(elapsed_time % 60)

        print(f"\n{'=' * 80}")
        print(f"RE-EXECUTION COMPLETE")
        print(f"{'=' * 80}")
        print(f"Total clusters processed: {len(self.problematic_clusters)}")
        print(f"Fully successful re-runs: {successful_reruns}")
        print(f"Total time elapsed: {hours:02d}h {minutes:02d}m {seconds:02d}s")

        # Detailed results summary
        print(f"\nDetailed Results:")
        for cluster_name, (base_success, llm_success) in sorted(results.items()):
            status = "✓" if (base_success and llm_success) else "✗"
            base_status = "✓" if base_success else "✗"
            llm_status = "✓" if llm_success else "✗"
            print(f"  {status} {cluster_name}: base={base_status}, llm={llm_status}")

        return results

    def save_rerun_report(
        self, results: Dict[str, Tuple[bool, bool]], output_path: Path
    ) -> None:
        """
        Save a report of the re-execution results

        Args:
            results: Dictionary mapping cluster names to success tuples
            output_path: Path where to save the report
        """
        report_data = {
            "metadata": {
                "generation_date": datetime.now().isoformat(),
                "source_outlier_report": str(self.outlier_report_path),
                "num_executions": self.num_executions,
                "max_workers": self.max_workers,
                "dry_run": self.dry_run,
                "total_clusters_rerun": len(results),
            },
            "results": {},
        }

        for cluster_name, (base_success, llm_success) in results.items():
            report_data["results"][cluster_name] = {
                "base_success": base_success,
                "llm_success": llm_success,
                "overall_success": base_success and llm_success,
                "outlier_details": self.get_cluster_outlier_details(cluster_name),
            }

        # Calculate summary statistics
        successful_count = sum(1 for bs, ls in results.values() if bs and ls)
        report_data["summary"] = {
            "total_clusters": len(results),
            "fully_successful": successful_count,
            "partially_successful": len(results) - successful_count,
            "success_rate": (successful_count / len(results) * 100) if results else 0,
        }

        # Save to file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        print(f"\nRe-execution report saved to: {output_path}")


def main():
    """Main entry point for the script"""
    parser = argparse.ArgumentParser(
        description="Re-execute tests on clusters with improvement outliers",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Re-run tests for all outlier clusters (default settings)
  python rerun_outlier_clusters.py --outlier-report metrics/outlier_reports/outliers_report_20251017_105325.json

  # Dry run to see what would be executed
  python rerun_outlier_clusters.py --outlier-report report.json --dry-run

  # Run with more executions and parallel workers
  python rerun_outlier_clusters.py --outlier-report report.json --num-executions 10 --max-workers 8

  # Skip backup of existing results
  python rerun_outlier_clusters.py --outlier-report report.json --no-backup

  # Specify custom paths
  python rerun_outlier_clusters.py \\
      --outlier-report /path/to/report.json \\
      --run-tests-script /path/to/run_tests_on_cluster.py \\
      --output /path/to/rerun_report.json
        """,
    )

    parser.add_argument(
        "--outlier-report",
        type=str,
        required=True,
        help="Path to the outlier report JSON file generated by find_improvement_outliers.py",
    )

    parser.add_argument(
        "--run-tests-script",
        type=str,
        default=None,
        help="Path to run_tests_on_cluster.py (default: auto-detect in same directory)",
    )

    parser.add_argument(
        "--num-executions",
        type=int,
        default=5,
        help="Number of times to execute each cluster (default: 5)",
    )

    parser.add_argument(
        "--max-workers",
        type=int,
        default=4,
        help="Number of parallel workers for test execution (default: 4)",
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be done without actually executing tests",
    )

    parser.add_argument(
        "--no-backup",
        action="store_true",
        help="Do not backup existing results before re-execution",
    )

    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="Output path for re-execution report (default: metrics/outlier_reports/rerun_report_YYYYMMDD_HHMMSS.json)",
    )

    args = parser.parse_args()

    # Determine run_tests_on_cluster.py path
    if args.run_tests_script:
        run_tests_script = Path(args.run_tests_script)
    else:
        # Auto-detect in same directory as this script
        run_tests_script = Path(__file__).parent / "run_tests_on_cluster.py"

    if not run_tests_script.exists():
        print(f"Error: Test runner script not found: {run_tests_script}")
        print("Please specify the path using --run-tests-script")
        sys.exit(1)

    # Determine outlier report path
    outlier_report_path = METRICS_DIR_FILEPATH / "outlier_reports" / args.outlier_report

    # Create rerunner instance
    rerunner = OutlierClusterRerunner(
        outlier_report_path=outlier_report_path,
        run_tests_script=run_tests_script,
        num_executions=args.num_executions,
        max_workers=args.max_workers,
        dry_run=args.dry_run,
        backup_results=not args.no_backup,
    )

    # Load outlier report
    if not rerunner.load_outlier_report():
        sys.exit(1)

    # Extract problematic clusters
    clusters = rerunner.extract_problematic_clusters()
    if not clusters:
        print("No problematic clusters found in report")
        sys.exit(0)

    # Ask for confirmation unless in dry-run mode
    if not args.dry_run:
        print(f"\n{'=' * 80}")
        print(f"CONFIRMATION REQUIRED")
        print(f"{'=' * 80}")
        print(f"About to re-execute tests for {len(clusters)} clusters")
        print("This will:")
        if rerunner.backup_results:
            print(f"  1. Backup existing result files to src/backup_executions")
        print(f"  2. Run {args.num_executions} base code executions per cluster")
        print(
            f"  3. Run {args.num_executions} LLM code executions per cluster (all prompt versions)"
        )
        print("  4. Overwrite existing result files with new execution data")
        print(
            f"\nEstimated time: This may take several hours depending on cluster complexity"
        )

        response = input("\nDo you want to proceed? (yes/no): ").strip().lower()
        if response not in ["yes", "y"]:
            print("Re-execution cancelled by user")
            sys.exit(0)

    # Execute re-runs
    results = rerunner.rerun_all_problematic_clusters()

    # Determine output path
    if args.output:
        output_path = Path(args.output)
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        outlier_reports_dir = METRICS_DIR_FILEPATH / "outlier_reports"
        output_path = outlier_reports_dir / f"rerun_report_{timestamp}.json"

    # Save report
    rerunner.save_rerun_report(results, output_path)

    print("\n" + "=" * 80)
    print("Re-execution process complete!")
    print(f"Report saved to: {output_path}")
    print(f"\nNext steps:")
    print(f"  1. Run find_improvement_outliers.py again to check if outliers persist")
    print(f"  2. Compare the new outlier counts with the original report")
    print(f"  3. Investigate any remaining persistent outliers")
    print("=" * 80)


if __name__ == "__main__":
    main()


# python3 rerun_outlier_clusters.py --outlier-report outliers_report_20251018_160045.json --num-executions 5 --max-workers 4
