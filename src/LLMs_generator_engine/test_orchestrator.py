"""
Unit tests for run_llm_and_similarity.py

Run with: python test_orchestrator.py
"""

import sys
import os
from pathlib import Path
import json
import tempfile
import shutil

# Add parent directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from LLMs_generator_engine.run_llm_and_similarity import (
    OrphanEntry,
    OrphanEntryDiscovery,
    ClusterJSONUpdater
)


def test_orphan_entry_creation():
    """Test OrphanEntry dataclass"""
    orphan = OrphanEntry(
        entry_id="c_test_user1",
        language="c",
        filename="test.c",
        code_snippet_path="c/test_user1/src/test.c",
        test_unit_path="c/test_user1/test/test_test.c",
        cluster_name="test",
        current_llm_count=5,
        entry_data={"id": "c_test_user1", "LLMs": []}
    )

    assert orphan.entry_id == "c_test_user1"
    assert orphan.language == "c"
    assert orphan.current_llm_count == 5
    assert orphan.cluster_name == "test"

    print("✓ Test orphan entry creation passed")


def test_find_orphans_in_cluster():
    """Test orphan discovery in cluster data"""
    # Create mock cluster data
    cluster_data = {
        "c": [
            {
                "id": "c_test1_user1",
                "filename": "test1.c",
                "codeSnippetFilePath": "c/test1_user1/src/test1.c",
                "testUnitFilePath": "c/test1_user1/test/test_test1.c",
                "LLMs": []  # Empty = orphan
            },
            {
                "id": "c_test2_user1",
                "filename": "test2.c",
                "codeSnippetFilePath": "c/test2_user1/src/test2.c",
                "testUnitFilePath": "c/test2_user1/test/test_test2.c",
                "LLMs": [{"type": "openAI"}, {"type": "claude"}]  # Partial = orphan
            },
            {
                "id": "c_test3_user1",
                "filename": "test3.c",
                "codeSnippetFilePath": "c/test3_user1/src/test3.c",
                "testUnitFilePath": "c/test3_user1/test/test_test3.c",
                "LLMs": [{"type": f"model{i}"} for i in range(12)]  # Complete = not orphan
            }
        ],
        "python": [
            {
                "id": "py_test_user1",
                "filename": "test.py",
                "LLMs": []  # Should be skipped (not C/C++)
            }
        ]
    }

    discovery = OrphanEntryDiscovery(target_languages=['c', 'cpp'])
    orphans = discovery._find_orphans_in_cluster(cluster_data, "test")

    assert len(orphans) == 2, f"Expected 2 orphans, found {len(orphans)}"
    assert orphans[0].entry_id == "c_test1_user1"
    assert orphans[0].current_llm_count == 0
    assert orphans[1].entry_id == "c_test2_user1"
    assert orphans[1].current_llm_count == 2

    print("✓ Test find orphans in cluster passed")


def test_extract_prompt_version():
    """Test prompt version extraction from filename"""
    updater = ClusterJSONUpdater()

    # Valid cases
    assert updater._extract_prompt_version("ChatGPT4_hamming_v1.c") == 1
    assert updater._extract_prompt_version("ClaudeSonnet4_test_v2.cpp") == 2
    assert updater._extract_prompt_version("GeminiFlash_example_v4.py") == 4

    # Invalid cases
    assert updater._extract_prompt_version("base_code.c") is None
    assert updater._extract_prompt_version("test_v5.txt") is None

    print("✓ Test extract prompt version passed")


def test_discover_llm_files_mock():
    """Test LLM file discovery with mock filesystem"""
    # Create temporary directory structure
    temp_dir = Path(tempfile.mkdtemp())

    try:
        # Create mock dataset structure
        exercise_dir = temp_dir / "c" / "hamming_exercism-user"
        exercise_dir.mkdir(parents=True)

        # Create src with base code
        src_dir = exercise_dir / "src"
        src_dir.mkdir()
        (src_dir / "hamming.c").write_text("int main() { return 0; }")

        # Create LLM directories with files
        for model in ["openAI", "claude", "gemini"]:
            model_dir = exercise_dir / model
            model_dir.mkdir()

            for version in [1, 2, 3, 4]:
                filename = f"{model}_hamming_v{version}.c"
                (model_dir / filename).write_text(f"// Generated by {model} v{version}")

        # Mock entry
        entry = {
            "id": "c_hamming_user1",
            "filename": "hamming.c",
            "codeSnippetFilePath": "c/hamming_exercism-user/src/hamming.c"
        }

        # Temporarily override DATASET_DIR
        from utility_dir import utility_paths
        original_dataset_dir = utility_paths.DATASET_DIR
        utility_paths.DATASET_DIR = temp_dir

        try:
            updater = ClusterJSONUpdater()
            llm_metadata = updater._discover_llm_files(entry, "hamming")

            # Should find 12 files (3 models × 4 versions)
            assert len(llm_metadata) == 12, f"Expected 12 LLM files, found {len(llm_metadata)}"

            # Check that all models are present
            models_found = set(meta["type"] for meta in llm_metadata)
            assert models_found == {"openAI", "claude", "gemini"}

            print("✓ Test discover LLM files mock passed")

        finally:
            utility_paths.DATASET_DIR = original_dataset_dir

    finally:
        # Cleanup
        shutil.rmtree(temp_dir, ignore_errors=True)


def test_scan_clusters_empty():
    """Test cluster scanning with no clusters"""
    # Create temporary empty directory
    temp_dir = Path(tempfile.mkdtemp())

    try:
        from utility_dir import utility_paths
        original_clusters_dir = utility_paths.CLUSTERS_DIR_FILEPATH
        utility_paths.CLUSTERS_DIR_FILEPATH = temp_dir

        try:
            discovery = OrphanEntryDiscovery()
            orphans = discovery.scan_clusters()

            assert len(orphans) == 0, f"Expected 0 orphans in empty dir, found {len(orphans)}"

            print("✓ Test scan clusters empty passed")

        finally:
            utility_paths.CLUSTERS_DIR_FILEPATH = original_clusters_dir

    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)


def test_cluster_json_updater_extract_version():
    """Test version extraction with various formats"""
    updater = ClusterJSONUpdater()

    test_cases = [
        ("ChatGPT4_hamming_v1.c", 1),
        ("ClaudeSonnet4_two_sum_v2.cpp", 2),
        ("GeminiFlash_test_v3.py", 3),
        ("Model_exercise_v4.js", 4),
        ("no_version.c", None),
        ("wrong_format_v5.txt", None),
        ("test_vX.c", None),
    ]

    for filename, expected in test_cases:
        result = updater._extract_prompt_version(filename)
        assert result == expected, f"For {filename}: expected {expected}, got {result}"

    print("✓ Test cluster JSON updater extract version passed")


def run_all_tests():
    """Run all tests"""
    print("\n" + "="*60)
    print("Running Orchestrator Unit Tests")
    print("="*60 + "\n")

    tests = [
        test_orphan_entry_creation,
        test_find_orphans_in_cluster,
        test_extract_prompt_version,
        test_discover_llm_files_mock,
        test_scan_clusters_empty,
        test_cluster_json_updater_extract_version,
    ]

    passed = 0
    failed = 0

    for test_func in tests:
        try:
            test_func()
            passed += 1
        except AssertionError as e:
            print(f"✗ {test_func.__name__} FAILED: {e}")
            failed += 1
        except Exception as e:
            print(f"✗ {test_func.__name__} ERROR: {e}")
            failed += 1

    print("\n" + "="*60)
    print(f"Test Results: {passed} passed, {failed} failed")
    print("="*60)

    return failed == 0


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
